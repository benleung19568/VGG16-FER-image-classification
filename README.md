# VGG16-FER-image-classification
README - VGG16Training.py / DataAugmentationForCNNtraining.py
This code builds a facial emotion detection system that classifies faces into seven emotions: angry, disgust, fear, happy, neutral, sad, and surprise. It uses deep learning with attention mechanisms to improve accuracy. We run two set of data, the MMAFEDB as real facial emotion image set and self-generated facial emotion image set (Assorted generated by SD and StyleGAN3). Then both set of data is augmented with 2 more versions to see whether the training / validation will be more accurate. For the image augmentation, the script creates an expanded dataset by generating multiple variations of facial emotion images. It works with both organized emotion-labeled folders and flat directories. Datasets are then used to train the CNN image classifier – VGG16 for facial emotion classification.
MMAFEDB dataset:
https://www.kaggle.com/datasets/mahmoudima/mma-facial-expression

VGG16Training.py
What the code does:
The program takes facial images, processes them through a neural network (primarily VGG16 with attention modules), and predicts the emotion shown. It includes a full pipeline from data loading to model evaluation with detailed visualizations.
Main features:
•	Custom VGG16 model with CBAM attention for better feature focus
•	Data augmentation (MixUp, CutMix) to improve generalization
•	Mixed precision training for faster processing
•	Visualization tools to understand model decisions
•	Support for real or synthetic datasets
Key parameters you can modify:
Data settings:
•	DATASET_TYPE: What data you're using ("Real", "Augmented", "Synthetic", "SyntheticAugmented")
•	INPUT_SIZE: Size to resize images (256×256)
•	FEATURE_EXTRACT: If true, freezes most layers except classifier
Training key settings: (Based on computing resource, for me it’s RTX 4080 laptop)
•	BATCH_SIZE: Images processed at once (32)
•	LEARNING_RATE: How quickly the model learns (0.001)
•	OPTIMIZER_TYPE: Algorithm for training ("sgd" or "adam")
•	NUM_EPOCHS: Total training rounds 25 epochs for the project
Model improvements:
•	SCHEDULER_TYPE: How learning rate changes over time ("cosine", "step", "plateau")
•	USE_EMA: Uses weight averaging for stability
•	EARLY_STOPPING: Stops training if not improving
•	USE_MIXUP: Blends images together during training
•	USE_LABEL_SMOOTHING: Makes target labels less rigid
Model choice:
•	MODEL_TYPE: Architecture to use ("vgg16", "vgg16_cbam", "resnet50")
Practical options:
•	DEBUG_MODE: Test with smaller dataset first
•	SAVE_FREQ: How often to save checkpoints (every 5 epochs)
•	TEST_MODE: Run evaluation only without training
•	VISUALIZE_CAM: Generate heatmaps showing what the model focuses on
The code handles everything from model setup to final evaluation, with detailed tracking of performance metrics throughout training.

DataAugmentationForCNNtraining.py
What the code does:
The program takes a folder of facial images, makes copies with random visual modifications, and saves them to help train more robust emotion recognition models.
How it works:
1.	It detects if the images are organized by emotion folders (happy, sad, etc.)
2.	Copies all original images to the output folder
3.	For each image, creates multiple augmented versions with random modifications
4.	Saves all augmented images with clear naming
Key features:
•	Works with common emotion categories (happy, sad, fear, angry, neutral, surprise, disgust)
•	Applies varied image transformations using both PIL and OpenCV
•	Preserves original images while adding augmented versions
•	Shows progress bars for each step
Augmentation types:
Using PIL library:
•	Brightness adjustments
•	Contrast changes
•	Horizontal flips
•	Small rotations
•	Sharpness adjustments
Using OpenCV:
•	Gaussian blur
•	Rotation with scaling
•	Horizontal flips
•	Slight noise addition
•	Perspective transformations
Usage:
The script will ask for:
•	How many augmented versions to create per image
•	Automatically handles folder structures
It's designed to work with facial emotion datasets by applying transformations that don't distort the emotional content of the faces while creating useful variations for training.
